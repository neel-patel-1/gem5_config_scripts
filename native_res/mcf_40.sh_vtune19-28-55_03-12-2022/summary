Elapsed Time: 1175.393s
    Clockticks: 138,049,694,364,228
    Instructions Retired: 39,537,789,485,255
    CPI Rate: 3.492
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    MUX Reliability: 1.000
    Retiring: 13.2% of Pipeline Slots
        Light Operations: 12.2% of Pipeline Slots
            FP Arithmetic: 0.0% of uOps
                FP x87: 0.0% of uOps
                FP Scalar: 0.0% of uOps
                FP Vector: 0.0% of uOps
            Memory Operations: 5.5% of Pipeline Slots
            Fused Instructions: 1.8% of Pipeline Slots
            Non Fused Branches: 1.2% of Pipeline Slots
            Nop Instructions: 0.4% of Pipeline Slots
            Other: 3.4% of Pipeline Slots
        Heavy Operations: 1.0% of Pipeline Slots
            Microcode Sequencer: 0.4% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
                CISC: 0.4% of Pipeline Slots
    Front-End Bound: 17.1% of Pipeline Slots
     | Issue: A significant portion of Pipeline Slots is remaining empty due to
     | issues in the Front-End.
     | 
     | Tips:  Make sure the code working size is not too large, the code layout
     | does not require too many memory accesses per cycle to get enough
     | instructions for filling four pipeline slots, or check for microcode
     | assists.
     |
        Front-End Latency: 11.9% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end latency issues, such as instruction-cache
         | misses, ITLB misses or fetch stalls after a branch misprediction. In
         | such cases, the front-end delivers no uOps.
         |
            ICache Misses: 0.1% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 5.2% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Branch
             | Resteers. Branch Resteers estimate the Front-End delay in
             | fetching operations from corrected path, following all sorts of
             | mispredicted branches. For example, branchy code with lots of
             | mispredictions might get categorized as Branch Resteers. Note the
             | value of this node may overlap its siblings.
             |
                Mispredicts Resteers: 4.6% of Clockticks
                 | A significant fraction of cycles could be stalled due to
                 | Branch Resteers as a result of Branch Misprediction at
                 | execution stage.
                 |
                Clears Resteers: 0.0% of Clockticks
                Unknown Branches: 0.6% of Clockticks
            DSB Switches: 6.2% of Clockticks
             | Issue: A significant portion of cycles is spent switching from
             | the DSB to the MITE.  This may happen if a hot code region is too
             | large to fit into the DSB.
             | 
             | Tips: Consider changing code layout (for example, via profile-
             | guided optimization) to help your hot regions fit into the DSB.
             | 
             | See the "Optimization for Decoded ICache" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            Length Changing Prefixes: 0.0% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Length
             | Changing Prefixes (LCPs).
             | 
             | Tips: To avoid this issue, use proper compiler flags. Intel
             | Compiler enables these flags by default.
             | 
             | See the "Length-Changing Prefixes (LCP)" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            MS Switches: 0.4% of Clockticks
        Front-End Bandwidth: 5.2% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end bandwidth issues, such as inefficiencies in
         | the instruction decoders or code restrictions for caching in the DSB
         | (decoded uOps cache). In such cases, the front-end typically delivers
         | a non-optimal amount of uOps to the back-end.
         |
            Front-End Bandwidth MITE: 4.9% of Pipeline Slots
             | This metric represents a fraction of cycles during which CPU was
             | stalled due to the MITE fetch pipeline issues, such as
             | inefficiencies in the instruction decoders.
             |
            Front-End Bandwidth DSB: 2.1% of Pipeline Slots
            Front-End Bandwidth LSD: 0.0% of Pipeline Slots
            (Info) DSB Coverage: 45.6%
             | Issue: A significant fraction of uOps was not delivered by the
             | DSB (known as Decoded ICache or uOp Cache). This may happen if a
             | hot code region is too large to fit into the DSB.
             | 
             | Tips: Consider changing the code layout (for example, via
             | profile-guided optimization) to help your hot regions fit into
             | the DSB.
             | 
             | See the "Optimization for Decoded ICache" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            (Info) LSD Coverage: 0.0%
            (Info) DSB Misses Cost: 9.8% of Pipeline Slots
             | %DSB_Misses_CostIssueTextAll
             |
    Bad Speculation: 15.9% of Pipeline Slots
     | A significant proportion of pipeline slots containing useful work are
     | being cancelled. This can be caused by mispredicting branches or by
     | machine clears. Note that this metric value may be highlighted due to
     | Branch Resteers issue.
     |
        Branch Mispredict: 15.8% of Pipeline Slots
         | Issue:: A significant proportion of branches are mispredicted,
         | leading to excessive wasted work or Backend stalls due to the machine
         | need to recover its state from a speculative path.
         | 
         | Tips:
         | 
         | 1. Identify heavily mispredicted branches and consider making your
         | algorithm more predictable or reducing the number of branches. You
         | can add more work to 'if' statements and move them higher in the code
         | flow for earlier execution. If using 'switch' or 'case' statements,
         | put the most commonly executed cases first. Avoid using virtual
         | function pointers for heavily executed calls.
         | 
         | 2. Use profile-guided optimization in the compiler.
         | 
         | See the Intel 64 and IA-32 Architectures Optimization Reference
         | Manual for general strategies to address branch misprediction issues.
         |
        Machine Clears: 0.1% of Pipeline Slots
    Back-End Bound: 53.7% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 40.8% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 4.1% of Clockticks
                DTLB Overhead: 100.0% of Clockticks
                    Load STLB Hit: 100.0% of Clockticks
                    Load STLB Miss: 4.1% of Clockticks
                Loads Blocked by Store Forwarding: 0.1% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.1% of Clockticks
                FB Full: 100.0% of Clockticks
            L2 Bound: 2.2% of Clockticks
            L3 Bound: 5.5% of Clockticks
             | This metric shows how often CPU was stalled on L3 cache, or
             | contended with a sibling Core. Avoiding cache misses (L2
             | misses/L3 hits) improves the latency and increases performance.
             |
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 17.9% of Clockticks
                 | This metric shows a fraction of cycles with demand load
                 | accesses that hit the L3 cache under unloaded scenarios
                 | (possibly L3 latency limited). Avoiding private cache misses
                 | (i.e. L2 misses/L3 hits) will improve the latency, reduce
                 | contention with sibling physical cores and increase
                 | performance. Note the value of this node may overlap with its
                 | siblings.
                 |
                SQ Full: 9.2% of Clockticks
                 | This metric measures fraction of cycles where the Super Queue
                 | (SQ) was full taking into account all request-types and both
                 | hardware SMT threads. The Super Queue is used for requests to
                 | access the L2 cache or to go out to the Uncore.
                 |
            DRAM Bound: 55.1% of Clockticks
             | This metric shows how often CPU was stalled on the main memory
             | (DRAM). Caching typically improves the latency and increases
             | performance.
             |
                Memory Bandwidth: 46.3% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | approaching bandwidth limits of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses to reduce cacheline transfers
                 | from/to memory using these possible techniques:
                 |     - Consume all bytes of each cacheline before it is
                 |       evicted (for example, reorder structure elements and
                 |       split non-hot ones).
                 |     - Merge compute-limited and bandwidth-limited loops.
                 |     - Use NUMA optimizations on a multi-socket system.
                 | 
                 | Note: software prefetches do not help a bandwidth-limited
                 | application.
                 |
                Memory Latency: 26.9% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | the latency of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses or interleave them with compute
                 | using such possible techniques as data layout re-structuring
                 | or software prefetches (through the compiler).
                 |
                    Local DRAM: 43.8% of Clockticks
                     | The number of CPU stalls on loads from the local memory
                     | exceeds the threshold. Consider caching data to improve
                     | the latency and increase the performance.
                     |
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 0.5% of Clockticks
                Store Latency: 5.8% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 4.5% of Clockticks
                    Store STLB Hit: 2.7% of Clockticks
                    Store STLB Hit: 1.7% of Clockticks
        Core Bound: 12.9% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.2% of Clockticks
            Port Utilization: 21.4% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 58.8% of Clockticks
                 | CPU executed no uOps on any execution port during a
                 | significant fraction of cycles. Long-latency instructions
                 | like divides may contribute to this issue. Check the Assembly
                 | view and Appendix C in the Optimization Guide to identify
                 | instructions with 5 or more cycles latency.
                 |
                    Serializing Operations: 0.2% of Clockticks
                        Slow Pause: 0.0% of Clockticks
                    Mixing Vectors: 0.0% of Clockticks
                Cycles of 1 Port Utilized: 15.3% of Clockticks
                Cycles of 2 Ports Utilized: 11.5% of Clockticks
                Cycles of 3+ Ports Utilized: 13.4% of Clockticks
                    ALU Operation Utilization: 12.1% of Clockticks
                        Port 0: 7.8% of Clockticks
                        Port 1: 8.6% of Clockticks
                        Port 5: 9.0% of Clockticks
                        Port 6: 23.1% of Clockticks
                    Load Operation Utilization: 15.3% of Clockticks
                        Port 2: 17.5% of Clockticks
                        Port 3: 18.3% of Clockticks
                    Store Operation Utilization: 11.7% of Clockticks
                        Port 4: 11.7% of Clockticks
                        Port 7: 6.5% of Clockticks
                Vector Capacity Usage (FPU): 6.3%
    Average CPU Frequency: 3.093 GHz
    Total Thread Count
    Paused Time: 0s
CPU Utilization (%)
    Physical Core Utilization: 95.9% (19.190 out of 20)
    Logical Core Utilization: 94.9% (37.976 out of 40)
Collection and Platform Info
    Application Command Line: ./native_spec/mcf_40.sh 
    Operating System: 5.13.0-44-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION="Ubuntu 20.04.5 LTS"
    Computer Name: castor.ittc.ku.edu
    Result Size: 3.9 MB 
    Collection start time: 01:28:56 04/12/2022 UTC
    Collection stop time: 01:48:32 04/12/2022 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 3.093 GHz
        Logical CPU Count: 40
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
