Elapsed Time: 0.015s
 | Application execution time is too short. Metrics data may be unreliable.
 | Consider reducing the sampling interval or increasing your application
 | execution time.
 |
    Clockticks: 1,390,753,211
    Instructions Retired: 1,719,019,281
    CPI Rate: 0.809
    MUX Reliability: 0.939
    Retiring: 0.0% of Pipeline Slots
        Light Operations: 45.3% of Pipeline Slots
            FP Arithmetic: 0.0% of uOps
                FP x87: 0.0% of uOps
                FP Scalar: 0.0% of uOps
                FP Vector: 0.0% of uOps
            Memory Operations: 37.9% of Pipeline Slots
            Fused Instructions: 0.0% of Pipeline Slots
            Non Fused Branches
            Nop Instructions
            Other
        Heavy Operations: 0.0% of Pipeline Slots
            Microcode Sequencer: 0.0% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
                CISC: 0.0% of Pipeline Slots
    Front-End Bound: 20.6% of Pipeline Slots
     | Issue: A significant portion of Pipeline Slots is remaining empty due to
     | issues in the Front-End.
     | 
     | Tips:  Make sure the code working size is not too large, the code layout
     | does not require too many memory accesses per cycle to get enough
     | instructions for filling four pipeline slots, or check for microcode
     | assists.
     |
        Front-End Latency: 0.0% of Pipeline Slots
            ICache Misses: 0.0% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 0.9% of Clockticks
                Mispredicts Resteers: 0.0% of Clockticks
                Clears Resteers: 0.0% of Clockticks
                Unknown Branches: 0.9% of Clockticks
            DSB Switches: 5.0% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 3.8% of Clockticks
        Front-End Bandwidth: 20.6% of Pipeline Slots
         | This metric represents a fraction of slots during which CPU was
         | stalled due to front-end bandwidth issues, such as inefficiencies in
         | the instruction decoders or code restrictions for caching in the DSB
         | (decoded uOps cache). In such cases, the front-end typically delivers
         | a non-optimal amount of uOps to the back-end.
         |
            Front-End Bandwidth MITE: 12.0% of Pipeline Slots
             | This metric represents a fraction of cycles during which CPU was
             | stalled due to the MITE fetch pipeline issues, such as
             | inefficiencies in the instruction decoders.
             |
            Front-End Bandwidth DSB: 0.0% of Pipeline Slots
            Front-End Bandwidth LSD: 0.0% of Pipeline Slots
            (Info) DSB Coverage: 27.4%
             | Issue: A significant fraction of uOps was not delivered by the
             | DSB (known as Decoded ICache or uOp Cache). This may happen if a
             | hot code region is too large to fit into the DSB.
             | 
             | Tips: Consider changing the code layout (for example, via
             | profile-guided optimization) to help your hot regions fit into
             | the DSB.
             | 
             | See the "Optimization for Decoded ICache" section in the Intel 64
             | and IA-32 Architectures Optimization Reference Manual.
             |
            (Info) LSD Coverage: 0.0%
            (Info) DSB Misses Cost: 20.6% of Pipeline Slots
             | %DSB_Misses_CostIssueTextAll
             |
    Bad Speculation: 49.1% of Pipeline Slots
     | A significant proportion of pipeline slots containing useful work are
     | being cancelled. This can be caused by mispredicting branches or by
     | machine clears. Note that this metric value may be highlighted due to
     | Branch Resteers issue.
     |
        Branch Mispredict: 43.9% of Pipeline Slots
         | Issue:: A significant proportion of branches are mispredicted,
         | leading to excessive wasted work or Backend stalls due to the machine
         | need to recover its state from a speculative path.
         | 
         | Tips:
         | 
         | 1. Identify heavily mispredicted branches and consider making your
         | algorithm more predictable or reducing the number of branches. You
         | can add more work to 'if' statements and move them higher in the code
         | flow for earlier execution. If using 'switch' or 'case' statements,
         | put the most commonly executed cases first. Avoid using virtual
         | function pointers for heavily executed calls.
         | 
         | 2. Use profile-guided optimization in the compiler.
         | 
         | See the Intel 64 and IA-32 Architectures Optimization Reference
         | Manual for general strategies to address branch misprediction issues.
         |
        Machine Clears: 5.2% of Pipeline Slots
         | Issue: A significant portion of execution time is spent handling
         | machine clears.
         | 
         | Tips: See the "Memory Disambiguation" section in the Intel 64 and
         | IA-32 Architectures Optimization Reference Manual.
         |
    Back-End Bound: 30.3% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 11.6% of Pipeline Slots
            L1 Bound: 23.6% of Clockticks
                DTLB Overhead: 0.0% of Clockticks
                    Load STLB Hit: 0.0% of Clockticks
                    Load STLB Miss: 0.5% of Clockticks
                Loads Blocked by Store Forwarding: 0.0% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 1.1% of Clockticks
                4K Aliasing: 0.0% of Clockticks
                FB Full: 31.7% of Clockticks
            L2 Bound
            L3 Bound: 0.0% of Clockticks
                Contested Accesses
                Data Sharing
                L3 Latency
                SQ Full: 0.1% of Clockticks
            DRAM Bound
                Memory Bandwidth: 0.3% of Clockticks
                Memory Latency: 0.0% of Clockticks
                    Local DRAM
                    Remote DRAM
                    Remote Cache
            Store Bound: 3.0% of Clockticks
                Store Latency: 1.0% of Clockticks
                False Sharing: 0.5% of Clockticks
                Split Stores: 0.2% of Clockticks
                DTLB Store Overhead: 7.8% of Clockticks
                    Store STLB Hit: 7.8% of Clockticks
                    Store STLB Hit: 0.0% of Clockticks
        Core Bound: 18.7% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.0% of Clockticks
            Port Utilization: 42.7% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 0.0% of Clockticks
                    Serializing Operations: 0.0% of Clockticks
                        Slow Pause: 0.0% of Clockticks
                    Mixing Vectors: 0.0% of Clockticks
                Cycles of 1 Port Utilized: 17.6% of Clockticks
                 | This metric represents cycles fraction where the CPU executed
                 | total of 1 uop per cycle on all execution ports. This can be
                 | due to heavy data-dependency among software instructions, or
                 | oversubscribing a particular hardware resource. In some other
                 | cases with high 1_Port_Utilized and L1 Bound, this metric can
                 | point to L1 data-cache latency bottleneck that may not
                 | necessarily manifest with complete execution starvation (due
                 | to the short L1 latency e.g. walking a linked list) - looking
                 | at the assembly can be helpful. Note that this metric value
                 | may be highlighted due to L1 Bound issue.
                 |
                Cycles of 2 Ports Utilized: 32.1% of Clockticks
                 | This metric represents cycles fraction CPU executed total of
                 | 2 uops per cycle on all execution ports. Tip: Loop
                 | Vectorization - most compilers feature auto-Vectorization
                 | options today- reduces pressure on the execution ports as
                 | multiple elements are calculated with same uop.
                 |
                Cycles of 3+ Ports Utilized: 84.0% of Clockticks
                 | This metric represents Core cycles fraction CPU executed
                 | total of 3 or more uops per cycle on all execution ports.
                 |
                    ALU Operation Utilization: 17.4% of Clockticks
                        Port 0: 33.0% of Clockticks
                        Port 1: 36.6% of Clockticks
                        Port 5: 0.0% of Clockticks
                        Port 6: 0.0% of Clockticks
                    Load Operation Utilization: 36.6% of Clockticks
                        Port 2: 53.0% of Clockticks
                        Port 3: 56.6% of Clockticks
                    Store Operation Utilization: 55.8% of Clockticks
                        Port 4: 55.8% of Clockticks
                        Port 7: 19.5% of Clockticks
                Vector Capacity Usage (FPU): 0.0%
    Average CPU Frequency: 3.043 GHz
    Total Thread Count
    Paused Time: 0s
CPU Utilization (%)
    Physical Core Utilization: 100.0% (20.000 out of 20)
    Logical Core Utilization: 78.4% (31.366 out of 40)
     | The metric value is low, which may signal a poor utilization of logical
     | CPU cores while the utilization of physical cores is acceptable. Consider
     | using logical cores, which in some cases can improve processor throughput
     | and overall performance of multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: ./native_spec/mcf_40.sh 
    Operating System: 5.13.0-44-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION="Ubuntu 20.04.5 LTS"
    Computer Name: castor.ittc.ku.edu
    Result Size: 3.9 MB 
    Collection start time: 01:26:40 04/12/2022 UTC
    Collection stop time: 01:26:40 04/12/2022 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 3.093 GHz
        Logical CPU Count: 40
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
